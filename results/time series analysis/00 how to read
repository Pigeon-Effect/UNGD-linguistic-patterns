Here is the heuristics for each language property investigated:

T1 - Swear Words: A publicly available dictionary of English-language swear words is used for this purpose. The dictionary can be found as a category on Wikimedia’s Wiktionary.  Three terms that contain jesus as well as the term spastic were removed from the list as it requires context understanding to distinguish their function as swearword from other meanings. Then the proportion of words that occur in both the corpus and the swear dictionary is determined and divided by the total number of words in the corpus.
T2 – Crisis Words: No publicly available dictionary of words related to crisis has been found. Therefore, ChatGPT 3.5 was tasked with the following: "create a dictionary with 100 entries for words with semantic similarity to the word crisis". Since the addition of a random temperature prevents deterministic results in this large language model, the list can be found in the GitHub repository.  The calculation is the same as for T1.
T3 – Numbers: Numbers are recognized using the Python package Regular expressions.  For this purpose, written-out numbers are recognized as well as numerical numbers. To prevent page-, paragraph or speaker numbering from distorting the proportion of actual content numbers extensive preprocessing has been applied to detect sequential numbers. The calculation is the same as for T1.
G1 – Self-Reference: Any use of the first-person singular is understood as self-reference. The calculation is the same as for T1.
G2 – Direct Address: Any use of the second-person singular is understood as a direct address. The calculation is the same as for T1.
G3 – Modal Adverbs: Modal adverbs express the speaker's degree of certainty about what is being said. These adverbs often convey notions like possibility, necessity, obligation, or likelihood, functioning similarly to modal verbs (like "can," "must," "might," etc.) but in an adverbial form. A dictionary of English modal adverbs was found on Wiktionary. The calculation is the same as for T1. 
G4 – Degree Adverbs: Degree adverbs modify adjectives, other adverbs, or verbs to indicate the intensity, extent, or degree of the quality or action described. A dictionary of English degree adverbs was found on Wiktionary. The calculation is the same as for T1. 
G5 – Negation: Includes signal words of rejection and negation and their abbreviations. The calculation is the same as for T1.
S1 – Sentence Length: Calculated by the average number of words per sentence.
S2 – Lexical Diversity: Lexical diversity calculates the ratio between types (unique words) and tokens (all words with duplicates). Since the text bodies are different in length and a text with increasing length also has an increasing probability of word repetition, it is necessary to have a fixed window size, to calculate the type-token ratio within and then calculate the averages over the entire corpus. This approach is called moving type-token ratio (MATTR). 
S3 – Readability: Readability is calculated by considering sentence length and word length with the readability Test of Flesch and Kincaid's. The result is a number between 0 and 100, where 100 is very easy to read and 0 is very difficult to read. 206.835-(1.015×TotalWords/TotalSentences )-(84.6×TotalSyllables/TotalWords )
S4 – Sentiment Polarity: Python’s TextBlob is used for the sentiment polarity. For this purpose, a dictionary-based method is triangulated with a rule-based method (e.g. to recognize negations). The result is a value between -1 (very negative) and 1 (very positive). 
S5 – Sentiment Subjectivity: The sentiment subjectivity is also calculated with Python's TextBlob. It calculates the absolute value for all polarities and returns a value between 0 (very objective) and 1 (very subjective).
C1 - 

